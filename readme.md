## PLN1 - Procesamiento del Lenguaje Natural I ##

Nombre: Jos√© Aviani

C√≥digo: a2103

### Desaf√≠os ###


Este repositorio re√∫ne los **cuatro desaf√≠os pr√°cticos** desarrollados para la materia **Procesamiento del Lenguaje Natural 1 (PLN1)**.

Cada desaf√≠o aborda un conjunto distinto de t√©cnicas y modelos fundamentales para el PLN moderno: desde vectorizaci√≥n cl√°sica basada en bolsa de palabras, pasando por modelos distribucionales como Word2Vec, hasta modelos de lenguaje recurrentes y finalmente arquitecturas encoder‚Äìdecoder para traducci√≥n autom√°tica (*machine translation*).

<img src="./imgs/i.png" width="900">

---

### üìåDesaf√≠o 1 ‚Äî Vectorizaci√≥n, Similitud y Na√Øve Bayes ###

[Notebook](./Desafio_1.ipynb)

Este desaf√≠o introduce la representaci√≥n tradicional **documento‚Äìt√©rmino**, el uso de **similitud coseno** para analizar vecinos m√°s cercanos y un clasificador **zero-shot** basado en prototipos.

Luego se entrenan modelos **Na√Øve Bayes** comparando configuraciones de vectorizadores y par√°metros, buscando maximizar **F1-macro**.

El mejor resultado provino de **ComplementNB + TF sin IDF**, destacando c√≥mo la frecuencia relativa del t√©rmino fue m√°s √∫til que su rareza global.

Finalmente, se eval√∫a la matriz **t√©rmino‚Äìdocumento** para estudiar similitudes entre palabras y sus agrupamientos sem√°nticos.

<img src="./imgs/d1.png" width="600">

---

### üìå Desaf√≠o 2 ‚Äî Word2Vec y An√°lisis de Embeddings ###

[Notebook](./Desafio_2/Desafio_2.ipynb)

En este trabajo se entrenan **embeddings Word2Vec** sobre un corpus de canciones, analizando t√©rminos **m√°s y menos similares** y visualizando el espacio vectorial mediante reducci√≥n a 2D.

Se estudian cl√∫steres sem√°nticos emergentes y tambi√©n efectos t√≠picos de corpus peque√±os (t√©rminos raros o ruidosos).

El desaf√≠o incluy√≥ resolver varios aspectos pr√°cticos de entorno y visualizaci√≥n en VS Code, permitiendo una exploraci√≥n m√°s completa de los embeddings entrenados.

<img src="./imgs/d2.jpeg" width="600">

---

### üìå Desaf√≠o 3 ‚Äî Modelos de Lenguaje a nivel caracteres ###

[Notebook](./Desafio_3/Desafio_3.ipynb)

Aqu√≠ se construye un **modelo de lenguaje** basado en **caracteres**, evaluado mediante **perplejidad** en validaci√≥n.

Se comparan dos arquitecturas:

* **LSTM profunda**, que logra menor perplejidad y secuencias m√°s coherentes,

* **SimpleRNN**, √∫til como baseline pero limitada para dependencias largas.

Se implementan estrategias de generaci√≥n (**greedy**, **beam search**, **sampling con temperatura**), observ√°ndose las cl√°sicas tensiones entre coherencia, diversidad y repetici√≥n.

<img src="./imgs/d3.png" width="600">

---

### üìå Desaf√≠o 4 ‚Äî Traductor Seq2Seq con LSTM ###

[Readme](./Desafio_4/readme.md)

[Notebook parte 1](./Desafio_4/Desafio_4_Keras_Parte1.ipynb)

[Notebook parte 2](./Desafio_4/Desafio_4_Keras_Parte2.ipynb)

Se replica y extiende un traductor **encoder‚Äìdecoder** con LSTM implementado en **Keras**.

Se experimenta con diferentes valores de **n_units**, embeddings pre-entrenados y estrategias de decodificaci√≥n (**greedy**, **sampling**, **beam search**).

Aunque el val_loss mejora ligeramente entre la Parte 1 y Parte 2, la calidad de traducci√≥n se mantiene similar, reflejando las limitaciones inherentes a los modelos seq2seq simples frente a oraciones largas y errores acumulativos.

<img src="./imgs/d4.png" width="600">

---

üîö Conclusiones Finales

Este repositorio refleja un recorrido por t√©cnicas fundamentales del procesamiento del lenguaje natural:

* Desde **vectorizaci√≥n cl√°sica** y modelos probabil√≠sticos simples,

* pasando por **embeddings distribucionales**,

* construyendo **modelos generativos recurrentes**,

* hasta llegar a un **sistema de traducci√≥n autom√°tica**.

Cada desaf√≠o permiti√≥ analizar fortalezas, limitaciones y decisiones de dise√±o dentro de un flujo completo de experimentaci√≥n cient√≠fica: preprocesamiento, modelado, evaluaci√≥n y conclusiones.

En caso que tengan cualquier duda [jose.aviani@gmail.com](jose.aviani@gmail.com).

### ¬°Gracias! ###